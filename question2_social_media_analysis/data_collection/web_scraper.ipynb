{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Web Scraper",
   "id": "c36b0e9bc86d8172"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-26T16:12:36.806609Z",
     "start_time": "2025-09-26T16:12:35.952697Z"
    }
   },
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "def scrape_books(base_url):\n",
    "    \"\"\"\n",
    "    Scrapes book data from all pages of a book catalog website.\n",
    "    \n",
    "    Args:\n",
    "        base_url (str): The base URL of the website to scrape\n",
    "        \n",
    "    Returns:\n",
    "        pandas.DataFrame: DataFrame containing book titles, prices, and ratings\n",
    "    \"\"\"\n",
    "    # Initialize list to store all book data\n",
    "    all_books = []\n",
    "    \n",
    "    # Start with the first page\n",
    "    page_url = f\"{base_url}/catalogue/page-1.html\"\n",
    "    page_num = 1\n",
    "\n",
    "    # Continue scraping until no more pages are found\n",
    "    while True:\n",
    "        try:\n",
    "            # Send HTTP GET request to the current page\n",
    "            response = requests.get(page_url)\n",
    "            response.raise_for_status()  # Raise an exception for bad status codes\n",
    "            \n",
    "            # Parse the HTML content using BeautifulSoup\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "            # Find all book containers on the current page\n",
    "            books_on_page = soup.find_all('article', class_='product_pod')\n",
    "            \n",
    "            # If no books found, we've reached the end\n",
    "            if not books_on_page:\n",
    "                break  # No more books found\n",
    "\n",
    "            # Extract data from each book on the current page\n",
    "            for book in books_on_page:\n",
    "                # Extract book title from the anchor tag's title attribute\n",
    "                title = book.h3.a['title']\n",
    "                \n",
    "                # Extract and convert price from text to float (remove £ symbol)\n",
    "                price = float(book.find('p', class_='price_color').text.replace('£', ''))\n",
    "                \n",
    "                # Map star rating classes to numeric values\n",
    "                rating_map = {'One': 1, 'Two': 2, 'Three': 3, 'Four': 4, 'Five': 5}\n",
    "                \n",
    "                # Extract rating class and convert to numeric rating\n",
    "                rating_class = book.find('p', class_='star-rating')['class'][1]\n",
    "                rating = rating_map.get(rating_class, 0)\n",
    "\n",
    "                # Add book data to our collection\n",
    "                all_books.append({'title': title, 'price': price, 'rating': rating})\n",
    "\n",
    "            # Check if there's a \"next\" button to continue to the next page\n",
    "            next_button = soup.find('li', class_='next')\n",
    "            if next_button:\n",
    "                # Increment page number and construct next page URL\n",
    "                page_num += 1\n",
    "                page_url = f\"{base_url}/catalogue/page-{page_num}.html\"\n",
    "                print(f\"Scraping page {page_num}...\")\n",
    "            else:\n",
    "                # No next button found, we've scraped all pages\n",
    "                break\n",
    "\n",
    "            # Add a small delay\n",
    "            time.sleep(1)  # Add a small delay\n",
    "\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            # Handle any network-related errors gracefully\n",
    "            print(f\"An error occurred: {e}\")\n",
    "            break\n",
    "\n",
    "    # Convert the list of book dictionaries to a pandas DataFrame\n",
    "    return pd.DataFrame(all_books)"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Run the Scrapper",
   "id": "41fef9cb470974ea"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-26T16:12:38.695260Z",
     "start_time": "2025-09-26T16:12:36.818296Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# Run the scraper\n",
    "df_books = scrape_books('http://books.toscrape.com')\n",
    "df_books.to_csv('books_data.csv', index=False)\n",
    "print(\"Scraping complete. Data saved to books_data.csv\")"
   ],
   "id": "56335b2cf3db1e31",
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'Â51.77'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[2], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# Run the scraper\u001B[39;00m\n\u001B[1;32m----> 2\u001B[0m df_books \u001B[38;5;241m=\u001B[39m scrape_books(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mhttp://books.toscrape.com\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m      3\u001B[0m df_books\u001B[38;5;241m.\u001B[39mto_csv(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mbooks_data.csv\u001B[39m\u001B[38;5;124m'\u001B[39m, index\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[0;32m      4\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mScraping complete. Data saved to books_data.csv\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "Cell \u001B[1;32mIn[1], line 46\u001B[0m, in \u001B[0;36mscrape_books\u001B[1;34m(base_url)\u001B[0m\n\u001B[0;32m     43\u001B[0m title \u001B[38;5;241m=\u001B[39m book\u001B[38;5;241m.\u001B[39mh3\u001B[38;5;241m.\u001B[39ma[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtitle\u001B[39m\u001B[38;5;124m'\u001B[39m]\n\u001B[0;32m     45\u001B[0m \u001B[38;5;66;03m# Extract and convert price from text to float (remove £ symbol)\u001B[39;00m\n\u001B[1;32m---> 46\u001B[0m price \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mfloat\u001B[39m(book\u001B[38;5;241m.\u001B[39mfind(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mp\u001B[39m\u001B[38;5;124m'\u001B[39m, class_\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mprice_color\u001B[39m\u001B[38;5;124m'\u001B[39m)\u001B[38;5;241m.\u001B[39mtext\u001B[38;5;241m.\u001B[39mreplace(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m£\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m'\u001B[39m))\n\u001B[0;32m     48\u001B[0m \u001B[38;5;66;03m# Map star rating classes to numeric values\u001B[39;00m\n\u001B[0;32m     49\u001B[0m rating_map \u001B[38;5;241m=\u001B[39m {\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mOne\u001B[39m\u001B[38;5;124m'\u001B[39m: \u001B[38;5;241m1\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mTwo\u001B[39m\u001B[38;5;124m'\u001B[39m: \u001B[38;5;241m2\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mThree\u001B[39m\u001B[38;5;124m'\u001B[39m: \u001B[38;5;241m3\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mFour\u001B[39m\u001B[38;5;124m'\u001B[39m: \u001B[38;5;241m4\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mFive\u001B[39m\u001B[38;5;124m'\u001B[39m: \u001B[38;5;241m5\u001B[39m}\n",
      "\u001B[1;31mValueError\u001B[0m: could not convert string to float: 'Â51.77'"
     ]
    }
   ],
   "execution_count": 2
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
